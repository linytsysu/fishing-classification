{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, VarianceThreshold, f_classif\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/hy_round1_train_20200102'\n",
    "test_path = '../data/hy_round1_testA_20200102'\n",
    "\n",
    "train_df_list = []\n",
    "for file_name in os.listdir(train_path):\n",
    "    df = pd.read_csv(os.path.join(train_path, file_name))\n",
    "    train_df_list.append(df)\n",
    "\n",
    "test_df_list = []\n",
    "for file_name in os.listdir(test_path):\n",
    "    df = pd.read_csv(os.path.join(test_path, file_name))\n",
    "    test_df_list.append(df)\n",
    "\n",
    "train_df = pd.concat(train_df_list)\n",
    "test_df = pd.concat(test_df_list)\n",
    "\n",
    "train_df['time'] = pd.to_datetime(train_df['time'], format='%m%d %H:%M:%S')\n",
    "test_df['time'] = pd.to_datetime(test_df['time'], format='%m%d %H:%M:%S')\n",
    "\n",
    "all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "group_list = []\n",
    "for ship_id, group in all_df.groupby('渔船ID'):\n",
    "    type_ = group['type'].values[0]\n",
    "    group = group.sort_values(by=['time'])\n",
    "    group = group[['time', 'x', 'y', '速度', '方向']]\n",
    "    group = group.set_index('time')\n",
    "    if (group.index[-1] - group.index[0]) < timedelta(days=3):\n",
    "        group = group.append(pd.DataFrame(index=[group.index[0] + timedelta(days=3)]), sort=False)\n",
    "    group = group.resample('10min').mean().ffill()\n",
    "    group['type'] = type_\n",
    "    group['渔船ID'] = ship_id\n",
    "    group['time'] = group.index.values\n",
    "    group = group.set_index(pd.Index(range(group.shape[0])))\n",
    "    group_list.append(group)\n",
    "new_df = pd.concat(group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3482016, 7), (3897000, 7))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.shape, new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generate_tsfresh():\n",
    "    train_df = pd.read_csv('./train_v2.csv')\n",
    "    X_train = train_df.drop(columns=['type'])\n",
    "    y_train = train_df['type']\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    test_df = pd.read_csv('./test_v2.csv')\n",
    "    X_test = test_df[X_train.columns]\n",
    "    \n",
    "    base_model =  lgb.LGBMClassifier(n_estimators=400, objective='multiclass')\n",
    "    base_model.fit(X_train.values, y_train)\n",
    "    \n",
    "    selected_columns = X_train.columns[np.argsort(base_model.feature_importances_)[::-1][:24]]\n",
    "    print(selected_columns)\n",
    "    \n",
    "    X_train = X_train[selected_columns].values\n",
    "\n",
    "    X_test = X_test[selected_columns]\n",
    "    for column in list(X_test.columns[X_test.isnull().sum() > 0]):\n",
    "        mean_val = X_test[column].mean()\n",
    "        X_test[column].fillna(mean_val, inplace=True)\n",
    "    X_test = X_test.values\n",
    "    \n",
    "    return X_train, le.inverse_transform(y_train), X_test, selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x__quantile__q_0.1', 'y__maximum', 'x__minimum', 'y__quantile__q_0.8',\n",
      "       'y__quantile__q_0.9', 'y__quantile__q_0.7', 'y__minimum',\n",
      "       'x__quantile__q_0.2', '速度__number_crossing_m__m_1',\n",
      "       'x__quantile__q_0.9', 'x__maximum', 'x__quantile__q_0.4',\n",
      "       'y__number_cwt_peaks__n_1', '方向__quantile__q_0.9',\n",
      "       '速度__agg_autocorrelation__f_agg_\"median\"__maxlag_40', 'x__median',\n",
      "       'y__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_1__w_20',\n",
      "       '速度__agg_autocorrelation__f_agg_\"var\"__maxlag_40', 'x__quantile__q_0.3',\n",
      "       'y__quantile__q_0.6', '速度__quantile__q_0.7',\n",
      "       'y__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.2__ql_0.0',\n",
      "       'y__fft_coefficient__coeff_66__attr_\"angle\"',\n",
      "       'x__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_0__w_2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_train_tsfresh, y_train, X_test_tsfresh, feature_tsfresh = feature_generate_tsfresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    exported_pipeline = make_pipeline(\n",
    "        make_union(\n",
    "            make_pipeline(\n",
    "                make_union(\n",
    "                    FunctionTransformer(copy),\n",
    "                    FunctionTransformer(copy)\n",
    "                ),\n",
    "                SelectPercentile(score_func=f_classif, percentile=18)\n",
    "            ),\n",
    "            FunctionTransformer(copy)\n",
    "        ),\n",
    "        StackingEstimator(estimator=SGDClassifier(alpha=0.01, eta0=0.1, fit_intercept=False, l1_ratio=1.0, learning_rate=\"constant\", loss=\"hinge\", penalty=\"elasticnet\", power_t=0.1)),\n",
    "        VarianceThreshold(threshold=0.05),\n",
    "        ExtraTreesClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.55, min_samples_leaf=1, min_samples_split=4, n_estimators=100)\n",
    "    )\n",
    "    set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "    return exported_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train_tsfresh], axis=1)\n",
    "X_test = np.concatenate([X_test_tsfresh], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_macroF1_lgb(truth, predictions):  \n",
    "    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', f1, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8990782336522005, 0.9169748183797771, 0.9219911473589092, 0.917974346596199, 0.9036324067282578]\n",
      "0.9119301905430687 0.00891323686006115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "model_list = []\n",
    "score_list = []\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    model = get_model()\n",
    "    eval_set = (X_train[test_index], y_train[test_index])\n",
    "    model.fit(X_train[train_index], y_train[train_index])\n",
    "    model_list.append(model)\n",
    "    score_list.append(f1_score(y_train[test_index], model.predict(X_train[test_index]), average='macro'))\n",
    "    \n",
    "print(score_list)\n",
    "print(np.mean(score_list), np.std(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['x_transformed'] = MinMaxScaler().fit_transform(new_df[['x']].values)\n",
    "new_df['y_transformed'] = MinMaxScaler().fit_transform(new_df[['y']].values)\n",
    "new_df['v_transformed'] = MinMaxScaler().fit_transform(new_df[['速度']].values)\n",
    "new_df['d_transformed'] = MinMaxScaler().fit_transform(new_df[['方向']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "label_data = []\n",
    "for ship_id, group in new_df.groupby('渔船ID'):\n",
    "    train_data.append(group[['x_transformed', 'y_transformed', 'v_transformed', 'd_transformed']].values)\n",
    "    label_data.append(group['type'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:7000]\n",
    "test_data = train_data[7000:]\n",
    "label_data = label_data[:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_data)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "y = keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/20\n",
      "5600/5600 - 21s - loss: 0.8576 - accuracy: 0.6650 - val_loss: 0.6899 - val_accuracy: 0.7200\n",
      "Epoch 2/20\n",
      "5600/5600 - 18s - loss: 0.6706 - accuracy: 0.7129 - val_loss: 0.6474 - val_accuracy: 0.7557\n",
      "Epoch 3/20\n",
      "5600/5600 - 19s - loss: 0.6350 - accuracy: 0.7171 - val_loss: 0.6674 - val_accuracy: 0.7093\n",
      "Epoch 4/20\n",
      "5600/5600 - 19s - loss: 0.6000 - accuracy: 0.7371 - val_loss: 0.6088 - val_accuracy: 0.7286\n",
      "Epoch 5/20\n",
      "5600/5600 - 19s - loss: 0.5834 - accuracy: 0.7402 - val_loss: 0.5947 - val_accuracy: 0.7536\n",
      "Epoch 6/20\n",
      "5600/5600 - 19s - loss: 0.5688 - accuracy: 0.7421 - val_loss: 0.5638 - val_accuracy: 0.7557\n",
      "Epoch 7/20\n",
      "5600/5600 - 19s - loss: 0.5611 - accuracy: 0.7400 - val_loss: 0.5904 - val_accuracy: 0.7493\n",
      "Epoch 8/20\n",
      "5600/5600 - 19s - loss: 0.5452 - accuracy: 0.7532 - val_loss: 0.5645 - val_accuracy: 0.7657\n",
      "Epoch 9/20\n",
      "5600/5600 - 19s - loss: 0.5459 - accuracy: 0.7575 - val_loss: 0.6032 - val_accuracy: 0.7429\n",
      "Epoch 10/20\n",
      "5600/5600 - 19s - loss: 0.5293 - accuracy: 0.7621 - val_loss: 0.5538 - val_accuracy: 0.7600\n",
      "Epoch 11/20\n",
      "5600/5600 - 19s - loss: 0.5185 - accuracy: 0.7659 - val_loss: 0.5605 - val_accuracy: 0.7529\n",
      "Epoch 12/20\n",
      "5600/5600 - 19s - loss: 0.5098 - accuracy: 0.7718 - val_loss: 0.5590 - val_accuracy: 0.7579\n",
      "Epoch 13/20\n",
      "5600/5600 - 19s - loss: 0.4938 - accuracy: 0.7725 - val_loss: 0.6277 - val_accuracy: 0.7321\n",
      "Epoch 14/20\n",
      "5600/5600 - 19s - loss: 0.4946 - accuracy: 0.7775 - val_loss: 0.5482 - val_accuracy: 0.7693\n",
      "Epoch 15/20\n",
      "5600/5600 - 19s - loss: 0.4632 - accuracy: 0.7918 - val_loss: 0.5614 - val_accuracy: 0.7607\n",
      "Epoch 16/20\n",
      "5600/5600 - 19s - loss: 0.4508 - accuracy: 0.7950 - val_loss: 0.5740 - val_accuracy: 0.7629\n",
      "Epoch 17/20\n",
      "5600/5600 - 19s - loss: 0.4402 - accuracy: 0.8036 - val_loss: 0.5667 - val_accuracy: 0.7714\n",
      "Epoch 18/20\n",
      "5600/5600 - 19s - loss: 0.4188 - accuracy: 0.8141 - val_loss: 0.5934 - val_accuracy: 0.7493\n",
      "Epoch 19/20\n",
      "5600/5600 - 19s - loss: 0.3968 - accuracy: 0.8214 - val_loss: 0.6091 - val_accuracy: 0.7621\n",
      "Epoch 20/20\n",
      "5600/5600 - 19s - loss: 0.3841 - accuracy: 0.8284 - val_loss: 0.6062 - val_accuracy: 0.7557\n",
      "0.6622577176789054\n",
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/20\n",
      "5600/5600 - 21s - loss: 0.8575 - accuracy: 0.6443\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-95c98b9a03e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                       total_epochs=1)\n\u001b[0m\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m    397\u001b[0m                                  prefix='val_')\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Projects/fishing-classification/env/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, (train_index, valid_index) in enumerate(kf.split(X, y)):\n",
    "    X_train, X_valid, y_train, y_valid = X[train_index], X[valid_index], y[train_index], y[valid_index]\n",
    "    \n",
    "    inputs = keras.Input(shape=(433, 4))\n",
    "    bilstm = keras.layers.Bidirectional(keras.layers.LSTM(30, return_sequences=True))(inputs)\n",
    "    x = keras.layers.Flatten()(bilstm)\n",
    "    x = keras.layers.Dense(32, activation='relu')(x)\n",
    "    output = keras.layers.Dense(3, activation='softmax')(x)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=20, verbose=2)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    y_valid = np.argmax(y_valid, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    valid_score = f1_score(y_valid, y_pred, average='macro')\n",
    "    print(valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    渔船ID             x             y   速度   方向                time type\n",
      "0   1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-17 10:31:37   刺网\n",
      "1   1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-17 09:58:20   刺网\n",
      "2   1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-17 08:27:46   刺网\n",
      "3   1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-17 07:57:58   刺网\n",
      "4   1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-17 03:57:26   刺网\n",
      "..   ...           ...           ...  ...  ...                 ...  ...\n",
      "71  1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-14 19:49:01   刺网\n",
      "72  1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-14 19:18:16   刺网\n",
      "73  1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-14 18:48:06   刺网\n",
      "74  1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-14 18:18:00   刺网\n",
      "75  1709  6.165599e+06  5.202660e+06  0.0  360 1900-11-14 17:48:16   刺网\n",
      "\n",
      "[76 rows x 7 columns]\n",
      "     渔船ID             x             y   速度   方向                time type\n",
      "0    3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-23 23:48:27   刺网\n",
      "1    3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-23 23:18:42   刺网\n",
      "2    3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-23 22:48:27   刺网\n",
      "3    3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-23 22:18:32   刺网\n",
      "4    3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-23 21:50:05   刺网\n",
      "..    ...           ...           ...  ...  ...                 ...  ...\n",
      "124  3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-21 13:40:57   刺网\n",
      "125  3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-21 13:11:06   刺网\n",
      "126  3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-21 13:10:50   刺网\n",
      "127  3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-21 12:40:59   刺网\n",
      "128  3423  6.165599e+06  5.202660e+06  0.0  360 1900-11-21 12:10:49   刺网\n",
      "\n",
      "[129 rows x 7 columns]\n",
      "    渔船ID             x             y   速度   方向                time type\n",
      "0   6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-13 23:46:03   刺网\n",
      "1   6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-13 23:14:57   刺网\n",
      "2   6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-13 22:45:40   刺网\n",
      "3   6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-13 22:14:31   刺网\n",
      "4   6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-13 21:44:14   刺网\n",
      "..   ...           ...           ...  ...  ...                 ...  ...\n",
      "73  6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-11 16:07:02   刺网\n",
      "74  6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-11 15:36:42   刺网\n",
      "75  6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-11 15:08:17   刺网\n",
      "76  6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-11 14:37:22   刺网\n",
      "77  6605  6.165599e+06  5.202660e+06  0.0  360 1900-11-11 01:04:45   刺网\n",
      "\n",
      "[78 rows x 7 columns]\n",
      "     渔船ID             x             y   速度   方向                time type\n",
      "0    6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-20 23:40:12   刺网\n",
      "1    6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-20 23:39:11   刺网\n",
      "2    6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-20 23:09:17   刺网\n",
      "3    6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-20 23:09:04   刺网\n",
      "4    6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-20 22:40:04   刺网\n",
      "..    ...           ...           ...  ...  ...                 ...  ...\n",
      "105  6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-18 02:30:00   刺网\n",
      "106  6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-18 02:00:30   刺网\n",
      "107  6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-18 01:30:00   刺网\n",
      "108  6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-18 01:00:52   刺网\n",
      "109  6791  6.165599e+06  5.202660e+06  0.0  360 1900-11-18 00:30:21   刺网\n",
      "\n",
      "[110 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "for ship_id, group in all_df.groupby('渔船ID'):\n",
    "    if np.mean(group['速度'].values) == 0:\n",
    "        print(group)\n",
    "#     print(group[group['速度'] > 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "runing_df = all_df[all_df['速度'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = runing_df.drop(columns=['type'])\n",
    "y = runing_df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [35:13<00:00, 70.47s/it]  \n"
     ]
    }
   ],
   "source": [
    "from tsfresh import extract_features\n",
    "extracted_df = extract_features(df, column_id='渔船ID', column_sort='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
