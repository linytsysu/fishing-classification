{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from seglearn.base import TS_Data\n",
    "from seglearn.datasets import load_watch\n",
    "from seglearn.pipe import Pype\n",
    "from seglearn.transform import FeatureRep, SegmentX, SegmentXY\n",
    "\n",
    "from tsfresh import select_features, extract_features\n",
    "from parameters import fc_parameters_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/hy_round2_train_20200225'\n",
    "test_path = '../data/hy_round2_test_20200308'\n",
    "\n",
    "train_df_list = []\n",
    "for file_name in os.listdir(train_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(train_path, file_name))\n",
    "        train_df_list.append(df)\n",
    "\n",
    "test_df_list = []\n",
    "for file_name in os.listdir(test_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(test_path, file_name))\n",
    "        test_df_list.append(df)\n",
    "\n",
    "train_df = pd.concat(train_df_list)\n",
    "test_df = pd.concat(test_df_list)\n",
    "\n",
    "train_df['time'] = pd.to_datetime(train_df['time'], format='%m%d %H:%M:%S')\n",
    "test_df['time'] = pd.to_datetime(test_df['time'], format='%m%d %H:%M:%S')\n",
    "\n",
    "all_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8166\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "id_list = []\n",
    "for ship_id, group in all_df.groupby('渔船ID'):\n",
    "    X.append(group[['lat', 'lon', '速度', '方向', 'time']])\n",
    "    y.append(group['type'].values[0])\n",
    "    id_list.append(ship_id)\n",
    "print(len(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yitinglin/Projects/fishing-classification/env/lib/python3.7/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pype = Pype([('segment', SegmentX(width=72, overlap=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pype = pype.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_list = []\n",
    "df_list = []\n",
    "for ship_id, group in all_df.groupby('渔船ID'):\n",
    "    sample = group[['lat', 'lon', '速度', '方向', 'time']].values\n",
    "    transform_result = pype.transform([sample])[0]\n",
    "\n",
    "    if transform_result.shape[0] == 0:\n",
    "        seg_df = pd.DataFrame(sample, columns=['lat', 'lon', '速度', '方向', 'time'])\n",
    "        seg_df['渔船ID'] = len(df_list)\n",
    "        seg_df['type'] = group['type'].values[0]\n",
    "        df_list.append(seg_df)\n",
    "        shape_list.append(1)        \n",
    "    else:\n",
    "        for seg in transform_result:\n",
    "            seg_df = pd.DataFrame(seg, columns=['lat', 'lon', '速度', '方向', 'time'])\n",
    "            seg_df['渔船ID'] = len(df_list)\n",
    "            seg_df['type'] = group['type'].values[0]\n",
    "            df_list.append(seg_df)\n",
    "        shape_list.append(transform_result.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_df = pd.concat(df_list, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_df.to_csv('help.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_df = pd.read_csv('help.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_all_df.drop(columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 40/40 [06:01<00:00,  9.05s/it]\n"
     ]
    }
   ],
   "source": [
    "extracted_df = extract_features(df, column_id='渔船ID', column_sort='time',\n",
    "                                n_jobs=8, kind_to_fc_parameters=fc_parameters_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_all_df.groupby('渔船ID').agg(x_min=('lat', 'min'), x_max=('lat', 'max'),\n",
    "            y_min=('lon', 'min'), y_max=('lon', 'max'))\n",
    "extracted_df['x_max-x_min'] = new_df['x_max'] - new_df['x_min']\n",
    "extracted_df['y_max-y_min'] = new_df['y_max'] - new_df['y_min']\n",
    "extracted_df['x_max-y_min'] = new_df['x_max'] - new_df['y_min']\n",
    "extracted_df['y_max-x_min'] = new_df['y_max'] - new_df['x_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yitinglin/Projects/fishing-classification/env/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/yitinglin/Projects/fishing-classification/env/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/yitinglin/Projects/fishing-classification/env/lib/python3.7/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "extracted_df['slope'] = extracted_df['y_max-y_min'] / np.where(extracted_df['x_max-x_min']==0, 0.001, extracted_df['x_max-x_min'])\n",
    "extracted_df['area'] = extracted_df['x_max-x_min'] * extracted_df['y_max-y_min']\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def get_feature(arr):\n",
    "    feature = [np.max(arr), np.quantile(arr, 0.9), np.quantile(arr, 0.1),\n",
    "               np.quantile(arr, 0.75), np.quantile(arr, 0.25), np.mean(arr), np.std(arr),\n",
    "               np.median(arr),  np.std(arr) / np.mean(arr)]\n",
    "    feature.append(np.corrcoef(np.array([arr[:-1], arr[1:]]))[0, 1])\n",
    "    feature.append(skew(arr))\n",
    "    feature.append(kurtosis(arr))\n",
    "    return feature\n",
    "\n",
    "\n",
    "features = []\n",
    "for _, group in new_all_df.groupby('渔船ID'):\n",
    "    group = group.sort_values(by=['time'])\n",
    "    lat = group['lat'].values\n",
    "    lon = group['lon'].values\n",
    "    time_ = pd.to_datetime(group['time'], format='%Y-%m-%d %H:%M:%S').values\n",
    "    dire = group['方向'].values\n",
    "\n",
    "    speed_list = []\n",
    "    for i in range(lat.shape[0]):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        hour = (time_[i] - time_[i-1]) / np.timedelta64(1,'h')\n",
    "        dist = geodesic((lat[i-1], lon[i-1]), (lat[i ], lon[i]))\n",
    "        speed_list.append(dist.km / hour)\n",
    "\n",
    "    c = np.sum(np.cos(dire / 180 * np.pi)) / group.shape[0]\n",
    "    s = np.sum(np.sin(dire / 180 * np.pi)) / group.shape[0]\n",
    "    r = np.sqrt(c ** 2 + s ** 2)\n",
    "    theta = np.arctan(s / c)\n",
    "    angle_feature = [r, theta, np.sqrt(-2 * np.log(r))]\n",
    "\n",
    "    turn_list = []\n",
    "    for i in range(dire.shape[0]):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        turn = 1 - np.cos(dire[i-1] / 180 * np.pi - dire[i] / 180 * np.pi)\n",
    "        turn_list.append(turn * np.pi)\n",
    "    turn_list = np.array(turn_list)\n",
    "    c = np.sum(np.cos(turn_list)) / (group.shape[0] - 1)\n",
    "    s = np.sum(np.sin(turn_list)) / (group.shape[0] - 1)\n",
    "    r = np.sqrt(c ** 2 + s ** 2)\n",
    "    theta = np.arctan(s / c)\n",
    "    turn_feature =  [r, theta, np.sqrt(-2 * np.log(r))]\n",
    "\n",
    "    features.append(np.concatenate([get_feature(speed_list),\n",
    "                                    angle_feature[:1], turn_feature[:1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df_ = pd.concat([pd.DataFrame(np.array(features)), extracted_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for name, group in new_all_df.groupby('渔船ID'):\n",
    "    y.append(group.iloc[0]['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = extracted_df_.iloc[:np.sum(shape_list[:len(train_df_list)])]\n",
    "test_df = extracted_df_.iloc[np.sum(shape_list[:len(train_df_list)]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:train_df.shape[0]]\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yitinglin/Projects/fishing-classification/env/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_df['type'] = le.inverse_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./train.csv')\n",
    "test_df.to_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train.csv', index_col=0)\n",
    "X_train = train_df.drop(columns=['type']).values\n",
    "y_train = train_df['type'].values\n",
    "\n",
    "test_df = pd.read_csv('./test.csv', index_col=0)\n",
    "X_test = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train = imputer.fit_transform(pd.DataFrame(X_train).replace([np.inf, -np.inf], np.nan).values)\n",
    "X_test = imputer.fit_transform(pd.DataFrame(X_test).replace([np.inf, -np.inf], np.nan).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, VarianceThreshold, f_classif\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import copy\n",
    "from geopy.distance import geodesic\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tpot.builtins import StackingEstimator, ZeroCount\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    exported_pipeline = make_pipeline(\n",
    "        SelectPercentile(score_func=f_classif, percentile=48),\n",
    "        StackingEstimator(estimator=SGDClassifier(alpha=0.01, eta0=0.01, fit_intercept=False, l1_ratio=0.25, learning_rate=\"invscaling\", loss=\"modified_huber\", penalty=\"elasticnet\", power_t=10.0)),\n",
    "        ExtraTreesClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.6000000000000001, min_samples_leaf=1, min_samples_split=3, n_estimators=100)\n",
    "    )\n",
    "\n",
    "    set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "    return exported_pipeline\n",
    "\n",
    "\n",
    "def get_model_v2():\n",
    "    exported_pipeline = make_pipeline(\n",
    "        make_union(\n",
    "            make_pipeline(\n",
    "                make_union(\n",
    "                    FunctionTransformer(copy),\n",
    "                    FunctionTransformer(copy)\n",
    "                ),\n",
    "                SelectPercentile(score_func=f_classif, percentile=18)\n",
    "            ),\n",
    "            FunctionTransformer(copy)\n",
    "        ),\n",
    "        StackingEstimator(estimator=SGDClassifier(alpha=0.01, eta0=0.1, fit_intercept=False, l1_ratio=1.0, learning_rate=\"constant\", loss=\"hinge\", penalty=\"elasticnet\", power_t=0.1)),\n",
    "        VarianceThreshold(threshold=0.05),\n",
    "        ExtraTreesClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.55, min_samples_leaf=1, min_samples_split=4, n_estimators=100)\n",
    "    )\n",
    "    set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "    return exported_pipeline\n",
    "\n",
    "\n",
    "def get_model_v3():\n",
    "    exported_pipeline = make_pipeline(\n",
    "        make_union(\n",
    "            FunctionTransformer(copy),\n",
    "            FunctionTransformer(copy)\n",
    "        ),\n",
    "        RobustScaler(),\n",
    "        RFE(estimator=ExtraTreesClassifier(criterion=\"entropy\", max_features=0.25, n_estimators=100), step=0.6500000000000001),\n",
    "        StandardScaler(),\n",
    "        GradientBoostingClassifier(learning_rate=0.5, max_depth=9, max_features=0.05, min_samples_leaf=18, min_samples_split=3, n_estimators=100, subsample=0.9000000000000001)\n",
    "    )\n",
    "    # Fix random state for all the steps in exported pipeline\n",
    "    set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "    return exported_pipeline\n",
    "\n",
    "\n",
    "def get_model_v4():\n",
    "    exported_pipeline = make_pipeline(\n",
    "        StackingEstimator(estimator=XGBClassifier(learning_rate=0.001, max_depth=2, min_child_weight=17, n_estimators=100, nthread=1, subsample=0.8)),\n",
    "        ZeroCount(),\n",
    "        VarianceThreshold(threshold=0.2),\n",
    "        RFE(estimator=ExtraTreesClassifier(criterion=\"entropy\", max_features=0.15000000000000002, n_estimators=100), step=0.2),\n",
    "        GradientBoostingClassifier(learning_rate=0.5, max_depth=7, max_features=0.15000000000000002, min_samples_leaf=2, min_samples_split=3, n_estimators=100, subsample=1.0)\n",
    "    )\n",
    "    # Fix random state for all the steps in exported pipeline\n",
    "    set_param_recursive(exported_pipeline.steps, 'random_state', 37)\n",
    "    return exported_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(shape_idx):\n",
    "    start_idx = int(np.sum(shape_list[:shape_idx]))\n",
    "    end_idx = start_idx + shape_list[shape_idx]\n",
    "    if shape_idx < len(train_df_list):\n",
    "        \n",
    "        return X_train[start_idx: end_idx], y_train[start_idx: end_idx]\n",
    "    else:\n",
    "        return X_test[start_idx: end_idx], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9320656913719013, 0.9333653722098739, 0.922073642372378, 0.928996168103867, 0.9358158716671782]\n",
      "0.9304633491450396\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=2019, shuffle=True)\n",
    "\n",
    "model_v1_list = []\n",
    "score_v1_list = []\n",
    "for train_index, test_index in kf.split(shape_list[:len(train_df_list)]):\n",
    "    train_data = []\n",
    "    y_data = []\n",
    "    for idx in train_index:\n",
    "        data = get_data(idx)\n",
    "        train_data.append(data[0])\n",
    "        y_data.append(data[1])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    y_data = np.concatenate(y_data, axis=0)\n",
    "    \n",
    "    model_v1 = get_model()\n",
    "    model_v1.fit(train_data, y_data)\n",
    "    model_v1_list.append(model_v1)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for idx in test_index:\n",
    "        data = get_data(idx)\n",
    "        proba = model_v1.predict_proba(data[0])\n",
    "        pred = np.argmax(np.sum(proba, axis=0) / proba.shape[0])\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(data[1][0])\n",
    "    score = f1_score(y_pred, y_true, average='macro')\n",
    "    score_v1_list.append(score)\n",
    "\n",
    "print(score_v1_list)\n",
    "print(np.mean(score_v1_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.938811206259881, 0.9424751751343262, 0.9360958999775758, 0.9312737752294025, 0.9171873243313445]\n",
      "0.9331686761865059\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=22, shuffle=True)\n",
    "\n",
    "model_v2_list = []\n",
    "score_v2_list = []\n",
    "for train_index, test_index in kf.split(shape_list[:len(train_df_list)]):\n",
    "    train_data = []\n",
    "    y_data = []\n",
    "    for idx in train_index:\n",
    "        data = get_data(idx)\n",
    "        train_data.append(data[0])\n",
    "        y_data.append(data[1])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    y_data = np.concatenate(y_data, axis=0)\n",
    "    \n",
    "    model_v2 = get_model_v2()\n",
    "    model_v2.fit(train_data, y_data)\n",
    "    model_v2_list.append(model_v2)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for idx in test_index:\n",
    "        data = get_data(idx)\n",
    "        proba = model_v2.predict_proba(data[0])\n",
    "        pred = np.argmax(np.sum(proba, axis=0) / proba.shape[0])\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(data[1][0])\n",
    "    score = f1_score(y_pred, y_true, average='macro')\n",
    "    score_v2_list.append(score)\n",
    "\n",
    "print(score_v2_list)\n",
    "print(np.mean(score_v2_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9342527588228555, 0.9335911664154944, 0.925678050344717, 0.9301315398036131, 0.9185788476021499]\n",
      "0.928446472597766\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=22, shuffle=True)\n",
    "\n",
    "model_v3_list = []\n",
    "score_v3_list = []\n",
    "for train_index, test_index in kf.split(shape_list[:len(train_df_list)]):\n",
    "    train_data = []\n",
    "    y_data = []\n",
    "    for idx in train_index:\n",
    "        data = get_data(idx)\n",
    "        train_data.append(data[0])\n",
    "        y_data.append(data[1])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    y_data = np.concatenate(y_data, axis=0)\n",
    "    \n",
    "    model_v3 = get_model_v3()\n",
    "    model_v3.fit(train_data, y_data)\n",
    "    model_v3_list.append(model_v3)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for idx in test_index:\n",
    "        data = get_data(idx)\n",
    "        proba = model_v3.predict_proba(data[0])\n",
    "        pred = np.argmax(np.sum(proba, axis=0) / proba.shape[0])\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(data[1][0])\n",
    "    score = f1_score(y_pred, y_true, average='macro')\n",
    "    score_v3_list.append(score)\n",
    "\n",
    "print(score_v3_list)\n",
    "print(np.mean(score_v3_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9146590144496635, 0.9381133327011759, 0.9273261455605576, 0.934466096365238, 0.9315785755064403]\n",
      "0.9292286329166151\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "model_v4_list = []\n",
    "score_v4_list = []\n",
    "for train_index, test_index in kf.split(shape_list[:len(train_df_list)]):\n",
    "    train_data = []\n",
    "    y_data = []\n",
    "    for idx in train_index:\n",
    "        data = get_data(idx)\n",
    "        train_data.append(data[0])\n",
    "        y_data.append(data[1])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    y_data = np.concatenate(y_data, axis=0)\n",
    "    \n",
    "    model_v4 = get_model_v4()\n",
    "    model_v4.fit(train_data, y_data)\n",
    "    model_v4_list.append(model_v4)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for idx in test_index:\n",
    "        data = get_data(idx)\n",
    "        proba = model_v4.predict_proba(data[0])\n",
    "        pred = np.argmax(np.sum(proba, axis=0) / proba.shape[0])\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(data[1][0])\n",
    "    score = f1_score(y_pred, y_true, average='macro')\n",
    "    score_v4_list.append(score)\n",
    "\n",
    "print(score_v4_list)\n",
    "print(np.mean(score_v4_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "def get_model_v5():\n",
    "    exported_pipeline = make_pipeline(\n",
    "        StackingEstimator(estimator=BernoulliNB(alpha=1.0, fit_prior=False)),\n",
    "        RFE(estimator=ExtraTreesClassifier(criterion=\"entropy\", max_features=0.45, n_estimators=100), step=0.6500000000000001),\n",
    "        RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=100)\n",
    "    )\n",
    "    # Fix random state for all the steps in exported pipeline\n",
    "    set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "    \n",
    "    return exported_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9356314676828159, 0.921100248842016, 0.9246274125616899, 0.9326227472975872, 0.9397098076802649]\n",
      "0.9307383368128747\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "model_v5_list = []\n",
    "score_v5_list = []\n",
    "for train_index, test_index in kf.split(shape_list[:len(train_df_list)]):\n",
    "    train_data = []\n",
    "    y_data = []\n",
    "    for idx in train_index:\n",
    "        data = get_data(idx)\n",
    "        train_data.append(data[0])\n",
    "        y_data.append(data[1])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    y_data = np.concatenate(y_data, axis=0)\n",
    "    \n",
    "    model_v5 = get_model_v5()\n",
    "    model_v5.fit(train_data, y_data)\n",
    "    model_v5_list.append(model_v5)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for idx in test_index:\n",
    "        data = get_data(idx)\n",
    "        proba = model_v5.predict_proba(data[0])\n",
    "        pred = np.argmax(np.sum(proba, axis=0) / proba.shape[0])\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(data[1][0])\n",
    "    score = f1_score(y_pred, y_true, average='macro')\n",
    "    score_v5_list.append(score)\n",
    "\n",
    "print(score_v5_list)\n",
    "print(np.mean(score_v5_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(len(train_df_list), len(shape_list)):\n",
    "    start_idx = int(np.sum(shape_list[len(train_df_list):i]))\n",
    "    sample = X_test[start_idx: start_idx+shape_list[i]]\n",
    "    result = []\n",
    "    for model in model_v1_list:\n",
    "        result.append(np.sum(model.predict_proba(sample), axis=0) / shape_list[i])\n",
    "\n",
    "    for model in model_v2_list:\n",
    "        result.append(np.sum(model.predict_proba(sample), axis=0) / shape_list[i])\n",
    "        \n",
    "    for model in model_v3_list:\n",
    "        result.append(np.sum(model.predict_proba(sample), axis=0) / shape_list[i])\n",
    "        \n",
    "    for model in model_v4_list:\n",
    "        result.append(np.sum(model.predict_proba(sample), axis=0) / shape_list[i])\n",
    "        \n",
    "    for model in model_v5_list:\n",
    "        result.append(np.sum(model.predict_proba(sample), axis=0) / shape_list[i])\n",
    "\n",
    "    pred.append(np.argmax(np.sum(result, axis=0) / 25))\n",
    "    \n",
    "pred_ = le.inverse_transform(pred)\n",
    "pd.DataFrame(pred_, index=id_list[len(train_df_list):]).to_csv('./result.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9322416671693042, 0.9188021631095404, 0.9283979360845503, 0.9363050033366979, 0.9456274067134883]\n",
      "0.9322748352827162\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "model_v5_list = []\n",
    "score_v5_list = []\n",
    "for train_index, test_index in kf.split(shape_list[:len(train_df_list)]):\n",
    "    train_data = []\n",
    "    y_data = []\n",
    "    for idx in train_index:\n",
    "        data = get_data(idx)\n",
    "        train_data.append(data[0])\n",
    "        y_data.append(data[1])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    y_data = np.concatenate(y_data, axis=0)\n",
    "    \n",
    "    model_v5 = RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.1, min_samples_leaf=1, min_samples_split=2, n_estimators=100)\n",
    "    model_v5.fit(train_data, y_data)\n",
    "    model_v5_list.append(model_v5)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for idx in test_index:\n",
    "        data = get_data(idx)\n",
    "        proba = model_v5.predict_proba(data[0])\n",
    "        pred = np.argmax(np.sum(proba, axis=0) / proba.shape[0])\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(data[1][0])\n",
    "    score = f1_score(y_pred, y_true, average='macro')\n",
    "    score_v5_list.append(score)\n",
    "\n",
    "print(score_v5_list)\n",
    "print(np.mean(score_v5_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9304487342201977, 0.9347071185153187, 0.9299680817263454, 0.9216429879638269, 0.9328207798040479]\n",
      "0.9299175404459474\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "\n",
    "model_v5_list = []\n",
    "score_v5_list = []\n",
    "for train_index, test_index in kf.split(shape_list[:len(train_df_list)]):\n",
    "    train_data = []\n",
    "    y_data = []\n",
    "    for idx in train_index:\n",
    "        data = get_data(idx)\n",
    "        train_data.append(data[0])\n",
    "        y_data.append(data[1])\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    y_data = np.concatenate(y_data, axis=0)\n",
    "\n",
    "    model_v5 = ExtraTreesClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.6000000000000001, min_samples_leaf=1, min_samples_split=3, n_estimators=100)\n",
    "    model_v5.fit(train_data, y_data)\n",
    "    model_v5_list.append(model_v5)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for idx in test_index:\n",
    "        data = get_data(idx)\n",
    "        proba = model_v5.predict_proba(data[0])\n",
    "        pred = np.argmax(np.sum(proba, axis=0) / proba.shape[0])\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(data[1][0])\n",
    "    score = f1_score(y_pred, y_true, average='macro')\n",
    "    score_v5_list.append(score)\n",
    "\n",
    "print(score_v5_list)\n",
    "print(np.mean(score_v5_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
