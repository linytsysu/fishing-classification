{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "label_df = pd.read_csv('label.csv', header=None)\n",
    "y_train = label_df[0].values\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generate_tsfresh():\n",
    "    train_df = pd.read_csv('../history/train.csv')\n",
    "    X_train = train_df.drop(columns=['type'])\n",
    "    y_train = train_df['type']\n",
    "\n",
    "    test_df = pd.read_csv('../history/testB.csv')\n",
    "    X_test = test_df[X_train.columns]\n",
    "\n",
    "    base_model = lgb.LGBMClassifier(n_estimators=500, objective='multiclass', num_leaves=63,\n",
    "                                    max_depth=7, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8)\n",
    "    base_model.fit(X_train.values, y_train, verbose=100)\n",
    "\n",
    "    selected_columns = X_train.columns[np.argsort(base_model.feature_importances_)[::-1][:35]]\n",
    "    selected_columns = selected_columns[~selected_columns.isin(['x__maximum', 'x__minimum', 'y__maximum', 'y__minimum'])]\n",
    "\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    selector = SelectPercentile(score_func=f_classif, percentile=50)\n",
    "    selector.fit(imputer.fit_transform(X_train.replace([np.inf, -np.inf], np.nan).values), y_train)\n",
    "    selected_features = X_train.columns[np.argsort(selector.scores_)[::-1][:5]]\n",
    "    selected_features = selected_features[~selected_features.isin(['x__maximum', 'x__minimum', 'y__maximum', 'y__minimum'])]\n",
    "\n",
    "#     selected_features = np.union1d(selected_columns, selected_features)\n",
    "    selected_features = selected_columns\n",
    "    print(selected_features)\n",
    "    \n",
    "    X_train = X_train[selected_features]\n",
    "    X_test = X_test[selected_features]\n",
    "\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x__quantile__q_0.1', 'y__quantile__q_0.9', 'y__quantile__q_0.8',\n",
      "       '速度__number_crossing_m__m_1', '速度__quantile__q_0.7',\n",
      "       '速度__agg_autocorrelation__f_agg_\"median\"__maxlag_40',\n",
      "       'y__quantile__q_0.7',\n",
      "       'y__percentage_of_reoccurring_datapoints_to_all_datapoints',\n",
      "       'y__quantile__q_0.1', 'x__quantile__q_0.2', 'x__quantile__q_0.3',\n",
      "       'x__quantile__q_0.9', '速度__ratio_beyond_r_sigma__r_2',\n",
      "       '方向__ar_coefficient__k_10__coeff_0', 'x__quantile__q_0.4',\n",
      "       '速度__quantile__q_0.6', 'y__quantile__q_0.4',\n",
      "       '方向__approximate_entropy__m_2__r_0.5', '方向__quantile__q_0.9',\n",
      "       'y__quantile__q_0.6', 'y__number_cwt_peaks__n_1',\n",
      "       '速度__count_above_mean', 'x__number_peaks__n_1',\n",
      "       '速度__linear_trend__attr_\"pvalue\"', 'y__quantile__q_0.3',\n",
      "       'y__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_1__w_20',\n",
      "       '速度__fft_coefficient__coeff_6__attr_\"real\"',\n",
      "       '方向__approximate_entropy__m_2__r_0.7',\n",
      "       '速度__agg_autocorrelation__f_agg_\"var\"__maxlag_40',\n",
      "       'y__approximate_entropy__m_2__r_0.1',\n",
      "       'x__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_0__w_2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_train_tsfresh, _, X_test_tsfresh = feature_generate_tsfresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "def feature_dv():\n",
    "    train_data_1 = pd.read_csv('train_preprocess_v1.csv')\n",
    "    test_data_1 = pd.read_csv('testB_preprocess_v1.csv')\n",
    "\n",
    "    base_model = lgb.LGBMClassifier(n_estimators=300, objective='multiclass', num_leaves=63,\n",
    "                                    max_depth=7, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8)\n",
    "    base_model.fit(train_data_1.values, y_train)\n",
    "#     imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "#     selector = SelectPercentile(score_func=f_classif, percentile=48)\n",
    "#     selector.fit(imputer.fit_transform(train_data_1.replace([np.inf, -np.inf], np.nan).values), y_train)\n",
    "#     selected_features = train_data_1.columns[np.argsort(selector.scores_)[::-1][:10]]\n",
    "    \n",
    "    selected_features = train_data_1.columns[np.argsort(base_model.feature_importances_)[::-1][:15]]\n",
    "    print(selected_features)\n",
    "   \n",
    "    return train_data_1[selected_features], test_data_1[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dist__sum_of_reoccurring_values', 'dist__quantile__q_0.6',\n",
      "       'speed__quantile__q_0.7', 'speed__number_cwt_peaks__n_5',\n",
      "       'dist__quantile__q_0.9',\n",
      "       'speed__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.6',\n",
      "       'dist__length', 'speed__quantile__q_0.8',\n",
      "       'dist__range_count__max_1__min_-1',\n",
      "       'speed__partial_autocorrelation__lag_4',\n",
      "       'speed__approximate_entropy__m_2__r_0.3',\n",
      "       'dist__longest_strike_below_mean',\n",
      "       'dist__percentage_of_reoccurring_datapoints_to_all_datapoints',\n",
      "       'dist__quantile__q_0.7', 'speed__linear_trend__attr_\"pvalue\"'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_train_dv, X_test_dv = feature_dv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_3 = pd.read_csv('train_preprocess_v3.csv')\n",
    "test_data_3 = pd.read_csv('testB_preprocess_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = np.concatenate([X_train_tsfresh.values, train_data_3.values, X_train_dv.values], axis=1)\n",
    "X_test_all = np.concatenate([X_test_tsfresh.values, test_data_3.values, X_test_dv.values], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold\n",
    "\n",
    "def get_model():\n",
    "    exported_pipeline = make_pipeline(\n",
    "        RFE(estimator=ExtraTreesClassifier(criterion=\"entropy\", max_features=0.8000000000000001, n_estimators=100),\n",
    "            step=0.1),\n",
    "        VarianceThreshold(threshold=0.001),\n",
    "        StandardScaler(),\n",
    "        StackingEstimator(estimator=SGDClassifier(alpha=0.001, eta0=0.001, fit_intercept=False,\n",
    "                                                  l1_ratio=1.0, learning_rate=\"invscaling\", loss=\"perceptron\",\n",
    "                                                  penalty=\"elasticnet\", power_t=0.5)),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, max_depth=7, max_features=0.15000000000000002,\n",
    "                                   min_samples_leaf=2, min_samples_split=2, n_estimators=400,\n",
    "                                   subsample=0.8500000000000001)\n",
    "    )\n",
    "    set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "    return exported_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9179583653014433, 0.9251995674453738, 0.9174200682067202, 0.909435453378657, 0.9056389968948304]\n",
      "0.915130490245405 0.00688690425619603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "X_train_var = imputer.fit_transform(pd.DataFrame(X_train_all).replace([np.inf, -np.inf], np.nan).values)\n",
    "X_test_var = imputer.fit_transform(pd.DataFrame(X_test_all).replace([np.inf, -np.inf], np.nan).values)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "model_list_2 = []\n",
    "score_list = []\n",
    "for train_index, test_index in kf.split(X_train_var):\n",
    "    model = get_model()\n",
    "    model.fit(X_train_var[train_index], y_train[train_index])\n",
    "    model_list_2.append(model)\n",
    "    score_list.append(f1_score(y_train[test_index], model.predict(X_train_var[test_index]), average='macro'))\n",
    "    \n",
    "print(score_list)\n",
    "print(np.mean(score_list), np.std(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's macroF1: 0.842398\n",
      "[200]\tvalid_0's macroF1: 0.85418\n",
      "[300]\tvalid_0's macroF1: 0.865635\n",
      "[400]\tvalid_0's macroF1: 0.863698\n",
      "[500]\tvalid_0's macroF1: 0.86921\n",
      "[600]\tvalid_0's macroF1: 0.873166\n",
      "[700]\tvalid_0's macroF1: 0.873004\n",
      "[800]\tvalid_0's macroF1: 0.876324\n",
      "[900]\tvalid_0's macroF1: 0.878369\n",
      "Early stopping, best iteration is:\n",
      "[761]\tvalid_0's macroF1: 0.880383\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's macroF1: 0.862143\n",
      "[200]\tvalid_0's macroF1: 0.870641\n",
      "[300]\tvalid_0's macroF1: 0.868517\n",
      "[400]\tvalid_0's macroF1: 0.869039\n",
      "[500]\tvalid_0's macroF1: 0.876344\n",
      "[600]\tvalid_0's macroF1: 0.876434\n",
      "[700]\tvalid_0's macroF1: 0.877585\n",
      "[800]\tvalid_0's macroF1: 0.878051\n",
      "[900]\tvalid_0's macroF1: 0.879198\n",
      "Early stopping, best iteration is:\n",
      "[724]\tvalid_0's macroF1: 0.87972\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's macroF1: 0.832019\n",
      "[200]\tvalid_0's macroF1: 0.854633\n",
      "[300]\tvalid_0's macroF1: 0.856882\n",
      "[400]\tvalid_0's macroF1: 0.860832\n",
      "[500]\tvalid_0's macroF1: 0.866036\n",
      "[600]\tvalid_0's macroF1: 0.86878\n",
      "[700]\tvalid_0's macroF1: 0.873562\n",
      "[800]\tvalid_0's macroF1: 0.872087\n",
      "[900]\tvalid_0's macroF1: 0.877469\n",
      "[1000]\tvalid_0's macroF1: 0.873844\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[900]\tvalid_0's macroF1: 0.877469\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's macroF1: 0.833939\n",
      "[200]\tvalid_0's macroF1: 0.851284\n",
      "[300]\tvalid_0's macroF1: 0.870735\n",
      "[400]\tvalid_0's macroF1: 0.880897\n",
      "[500]\tvalid_0's macroF1: 0.882788\n",
      "[600]\tvalid_0's macroF1: 0.885316\n",
      "[700]\tvalid_0's macroF1: 0.888801\n",
      "[800]\tvalid_0's macroF1: 0.888438\n",
      "Early stopping, best iteration is:\n",
      "[654]\tvalid_0's macroF1: 0.889582\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's macroF1: 0.848383\n",
      "[200]\tvalid_0's macroF1: 0.868583\n",
      "[300]\tvalid_0's macroF1: 0.867666\n",
      "[400]\tvalid_0's macroF1: 0.872745\n",
      "[500]\tvalid_0's macroF1: 0.870506\n",
      "[600]\tvalid_0's macroF1: 0.87543\n",
      "[700]\tvalid_0's macroF1: 0.878803\n",
      "[800]\tvalid_0's macroF1: 0.883162\n",
      "[900]\tvalid_0's macroF1: 0.887268\n",
      "[1000]\tvalid_0's macroF1: 0.885495\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[879]\tvalid_0's macroF1: 0.888646\n",
      "[0.8803828049546514, 0.8797204230133541, 0.8774690141189708, 0.8895816283498282, 0.88864608300967]\n",
      "0.8831599906892948 0.004965183580028443\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# def evaluate_macroF1_lgb(truth, predictions):  \n",
    "#     pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n",
    "#     f1 = f1_score(truth, pred_labels, average='macro')\n",
    "#     return ('macroF1', f1, True)\n",
    "\n",
    "# kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "\n",
    "# model_list_1 = []\n",
    "# score_list = []\n",
    "# for train_index, test_index in kf.split(X_train_all):\n",
    "#     model = lgb.LGBMClassifier(n_estimators=1000, objective='multiclass', num_leaves=63, metric='custom',\n",
    "#                                max_depth=7, learning_rate=0.027, subsample=0.8,\n",
    "#                                colsample_bytree=0.8, reg_lambda=1)\n",
    "#     eval_set = (X_train_all[test_index], y_train[test_index])\n",
    "#     model.fit(X=X_train_all[train_index], y=y_train[train_index], eval_metric=evaluate_macroF1_lgb,\n",
    "#               eval_set=eval_set, early_stopping_rounds=200, verbose=100)\n",
    "#     model_list_1.append(model)\n",
    "#     score_list.append(f1_score(y_train[test_index], model.predict(X_train_all[test_index]), average='macro'))\n",
    "    \n",
    "# print(score_list)\n",
    "# print(np.mean(score_list), np.std(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# def evaluate_macroF1_xgb(predictions, dtrain):\n",
    "#     labels = dtrain.get_label()\n",
    "#     pred_labels = predictions.reshape(len(np.unique(labels)), -1).argmax(axis=0)\n",
    "#     f1 = f1_score(labels, pred_labels, average='macro')\n",
    "#     return 'macroF1', 1-f1\n",
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "# kf = KFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "\n",
    "# model_list_2 = []\n",
    "# score_list = []\n",
    "# for train_index, test_index in kf.split(X_train_all):\n",
    "#     model = xgb.XGBClassifier(n_estimators=1000, objective='multi:softmax', num_leaves=63,\n",
    "#                               max_depth=7, learning_rate=0.035, subsample=0.8,\n",
    "#                               colsample_bytree=0.8, reg_lambda=1)\n",
    "#     eval_set = [(X_train_all[test_index], y_train[test_index])]\n",
    "#     model.fit(X_train_all[train_index], y_train[train_index], eval_set=eval_set,\n",
    "#               early_stopping_rounds=150, verbose=0)\n",
    "#     model_list_2.append(model)\n",
    "#     score_list.append(f1_score(y_train[test_index], model.predict(X_train_all[test_index]), average='macro'))\n",
    "    \n",
    "# print(score_list)\n",
    "# print(np.mean(score_list), np.std(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = None\n",
    "\n",
    "for model in model_list_2:\n",
    "    if result_list is None:\n",
    "        result_list = model.predict_proba(X_test_var)\n",
    "    else:\n",
    "        result_list = result_list + model.predict_proba(X_test_var)\n",
    "        \n",
    "# for model in model_list_2:\n",
    "#     result_list = result_list + model.predict_proba(X_test_all)\n",
    "    \n",
    "# result = le.inverse_transform(np.argmax(result_list / 5, axis=1))\n",
    "# pd.DataFrame(result, index=range(9000, 11000)).to_csv('result.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result_list).to_csv('result_prob.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
